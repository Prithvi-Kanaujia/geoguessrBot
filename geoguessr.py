# -*- coding: utf-8 -*-
"""geoguessr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16cJVO3VI1aOiLiN3aA3XCjYPSMFzmPPy
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load


# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
!pip3 install 'tensorflow-gpu==1.15'

import matplotlib.pyplot as plt
import pathlib
import numpy as np
import pandas as pd
import os
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from math import sqrt, ceil, floor
from PIL import Image
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print(tf.test.gpu_device_name())

!ls 'gdrive/Shareddrives/geoguess/Dataset/'

from google.colab import drive
drive.mount('/content/drive')

!ls 'drive/Shareddrives/geoguess/trial'

data_dir = pathlib.Path('drive/Shareddrives/geoguess/Dataset')
checkpoint_dir = 'drive/Shareddrives/geoguess/checkpoints'
if not os.path.exists(checkpoint_dir):
  os.makedirs(checkpoint_dir)

batch_size = 128
img_width = 1536
img_height = 662
epochs = 10

img_target_scale = .1
img_target_width = int(floor(img_width * img_target_scale))
img_target_height= int(floor(img_height * img_target_scale))

INTERPOLATION = "bilinear"
AUTO = tf.data.AUTOTUNE

print(data_dir)

def count_samples(path):
    size = len(str(path))
    country = str(path)[43:-7]
    # print(len(list(path.glob('*/'))))
    # if path.name == "Germany" or path.name == "Mexico":
    # return path.name, len(list(path.glob('*/')))
    return path.name, len(list(path.glob('*/')))
    # else:
    #   return path.name, 0

zipped_data_distribution = list(map(count_samples,data_dir.glob('*/')))
data_distribution = list(zip(zipped_data_distribution))

print(zipped_data_distribution)

"""# New section"""

labels = data_distribution[0]
number_of_samples = data_distribution[1]

"""##Â Data Distribution

"""

# Data
def sample_count(point):
    return point[1]

# Sorting the data
sorted_zipped_data_distribution = zipped_data_distribution
# sorted_zipped_data_distribution.sort(key=sample_count)

"""### Pictures per Country (original)"""

# Unzipping the data
data_distribution = list(zip(*sorted_zipped_data_distribution))
labels = data_distribution[0]
samples = data_distribution[1]


# Shaping Data to fit graph
grid_size = ceil(sqrt(len(samples)))**2 
pad = '-'

labels = list(labels) + ([pad] * (grid_size - len(labels)))
samples = list(samples) + ([0] * (grid_size - len(samples)))

samples = np.split(np.array(samples), sqrt(grid_size))

# Heatmap
fig, ax = plt.subplots()
im = ax.imshow(samples)
ax.set_title("Pictures per Country")
fig.set_size_inches(18.5, 15.5, forward=True)
fig.tight_layout()

# Color bar
cbar = ax.figure.colorbar(im, ax=ax)
cbar.ax.set_ylabel('Pictures', rotation=-90)

# Annotations
ax.tick_params(left=False, bottom=False)
ax.axes.xaxis.set_visible(False)
ax.axes.yaxis.set_visible(False)
for i in range(len(samples)):
    for j in range(len(samples[i])):
        label_index = i * len(samples) + j
        label = labels[label_index].replace(' ', '\n')
        value = samples[i][j]
        text = ax.text(j, i,label, ha="center", va="center", color="w")

plt.show()

"""### Pictures per Country (filtered)

1.   List item
2.   List item


We filter out to only keep the countries with `100 >= pictures <= 10000`
"""

filtered_zipped_data_distribution = [ (country, pictures) for country, pictures in zipped_data_distribution if 100 <= pictures and pictures <= 10000 ]

# Unzipping the data
data_distribution = list(zip(*filtered_zipped_data_distribution))
labels = data_distribution[0]
samples = data_distribution[1]


# Shaping Data to fit graph
grid_size = ceil(sqrt(len(samples)))**2 
pad = '-'

labels = list(labels) + ([pad] * (grid_size - len(labels)))
samples = list(samples) + ([0] * (grid_size - len(samples)))

samples = np.split(np.array(samples), sqrt(grid_size))

# Heatmap
fig, ax = plt.subplots()
im = ax.imshow(samples)
ax.set_title("Pictures per Country âˆˆ [100, 10000]")
fig.set_size_inches(18.5, 15.5, forward=True)
fig.tight_layout()

# Color bar
cbar = ax.figure.colorbar(im, ax=ax)
cbar.ax.set_ylabel('Pictures', rotation=-90)

# Annotations
ax.tick_params(left=False, bottom=False)
ax.axes.xaxis.set_visible(False)
ax.axes.yaxis.set_visible(False)
for i in range(len(samples)):
    for j in range(len(samples[i])):
        label_index = i * len(samples) + j
        label = labels[label_index].replace(' ', '\n')
        value = samples[i][j]
        text = ax.text(j, i,label, ha="center", va="center", color="w")

plt.show()

"""# Filter Dataset"""

# Copy filt
import shutil

working_dir = pathlib.Path('drive/Shareddrives/geoguess/Dataset')
filtered_dataset_dir = working_dir / 'filtered_dataset'
filtered_dataset_dir.mkdir(exist_ok=True)
print( filtered_zipped_data_distribution)
for country, picture_count in filtered_zipped_data_distribution:
    for picture_path in data_dir.glob(f'{country}/images/*'):
        target_picture_path = filtered_dataset_dir / picture_path.name
        shutil.copy(picture_path, target_picture_path)

"""# Load Dataset"""

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="training",
  seed=123,
  image_size=(img_target_height, img_target_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(img_target_height, img_target_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

"""# Pre-processing
1. downscale image to 50% of original size
2. filter out countries with less than 100 samples
3. filter out countries with more than 10000 samples
"""

def downscale_image(image, label):
#     image = tf.image.resize(image, [768, 331], antialias=True,)
    image = tf.image.resize(image, [img_target_width, img_target_height], antialias=True,)
    return image, label

# def for_sample_count(min, max):
#     print(f'filter for_sample_count [{min}, {max}]')
#     def include(picture, label):
#         print(picture, label)
#         return True
#     return include

AUTOTUNE = tf.data.AUTOTUNE

preprocessed_train_ds = train_ds.cache() \
    .shuffle(1000) \
    .map(downscale_image, num_parallel_calls=AUTO) \
    .prefetch(buffer_size=AUTOTUNE)

preprocessed_val_ds = val_ds.cache() \
    .map(downscale_image, num_parallel_calls=AUTO) \
    .prefetch(buffer_size=AUTOTUNE)

normalization_layer = layers.Rescaling(1./255)

normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))

num_classes = len(class_names)

model = Sequential([
  layers.Rescaling(1./255, input_shape=(img_target_height, img_target_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

model.compile(
    optimizer='adam',
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

model.summary()

"""Set up checkpoints for long training sessions."""

checkpoint_filepath = os.path.join(checkpoint_dir, "weights-{epoch:02d}.hdf5")
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    verbose=1,
    save_best_only=False,
    save_weights_only=True,
    mode='auto',
    save_freq=10 # every 10 batches
)

latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)

starting_epoch = 0
if latest_checkpoint:
  print(f"Loading weights from {latest_checkpoint}")
  model.load_weights(latest_checkpoint)
  starting_epoch = int(latest_checkpoint.split("-")[1])

print(starting_epoch)

starting_epoch = 4
latest_checkpoint = f"{checkpoint_dir}/weights-04.hdf5"
epochs = 4
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
  batch_size=batch_size,
  callbacks=[checkpoint_callback],
  initial_epoch = starting_epoch
)
model.save('drive/Shareddrives/geoguess/pretrained_2',save_format="h5")

model = keras.models.load_model('drive/Shareddrives/geoguess/pretrained_2')

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

import random
for x in range(5):

  # predict_image = random.choice(list(data_dir.glob("Canada/*")))
  predict_image = "/content/drive/Shareddrives/geoguess/Greece3.png"
  print(f"Image: {predict_image}")
  data = tf.keras.utils.img_to_array(tf.keras.utils.load_img(predict_image))
  data = tf.image.resize(data, [img_target_height, img_target_width])
  data = np.expand_dims(data, axis=0)  # Add an extra dimension for the batch
  
  predictions = model.predict(data)
  print(predictions)
  print(class_names)
  predicted_class_index = np.argmax(predictions[0])
  predicted_class_name = class_names[predicted_class_index]
  print(f"Predicted class: {predicted_class_name}")
  print(predictions[0][predicted_class_index])

